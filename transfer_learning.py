# -*- coding: utf-8 -*-
"""Transfer learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kGFho-kqJ3FdGuLR0qm6SlRX4Abcci1v

Clone Dataset from GitHub
"""

!git clone https://github.com/fruits-360/fruits-360-100x100.git

#List folders
!ls fruits-360-100x100

import os

# Paths to the dataset (adjust if folder name differs after cloning)
train_dir = "/content/fruits-360-100x100/Training"
test_dir = "/content/fruits-360-100x100/Test"

# Get class names
class_names = sorted(os.listdir(train_dir))
num_classes = len(class_names)

print(f"Number of classes: {num_classes}")
print("Example classes:", class_names[:10])

"""**EDA**


"""

import pandas as pd

train_counts = {cls: len(os.listdir(os.path.join(train_dir, cls))) for cls in class_names}
test_counts = {cls: len(os.listdir(os.path.join(test_dir, cls))) for cls in class_names}

eda_df = pd.DataFrame({
    "Class": class_names,
    "Train Images": [train_counts[cls] for cls in class_names],
    "Test Images": [test_counts[cls] for cls in class_names]
})

eda_df.head()

import matplotlib.pyplot as plt

plt.figure(figsize=(18,6))
eda_df.sort_values("Train Images", ascending=False).plot(
    x="Class", y=["Train Images", "Test Images"], kind="bar", figsize=(18,6)
)
plt.title("Class Distribution in Train/Test Sets")
plt.ylabel("Number of Images")
plt.xlabel("Fruit Class")
plt.xticks(rotation=90)
plt.show()

import random
import matplotlib.image as mpimg

sample_classes = random.sample(class_names, 5)  # Pick 5 random classes
plt.figure(figsize=(15, 5))

for i, cls in enumerate(sample_classes):
    img_name = random.choice(os.listdir(os.path.join(train_dir, cls)))
    img_path = os.path.join(train_dir, cls, img_name)
    img = mpimg.imread(img_path)
    plt.subplot(1, 5, i+1)
    plt.imshow(img)
    plt.title(cls)
    plt.axis("off")

plt.show()

"""**Data preprocessing & augmentation**"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Testing data generator (no augmentation)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(100, 100),
    batch_size=32,
    class_mode='categorical'
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(100, 100),
    batch_size=32,
    class_mode='categorical'
)

"""Implements three transfer learning models:

EfficientNetB0

ResNet50

DenseNet121

**build three tranfer learning models**
"""

from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.applications import EfficientNetB0, ResNet50, DenseNet121

def build_and_train_model(base_model_class, input_shape=(100, 100, 3), num_classes=131, model_name="Model"):
    print(f"\n==== Training {model_name} ====")

    # Load base model without top layer
    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)
    base_model.trainable = False  # Freeze base model

    # Add custom layers
    x = GlobalAveragePooling2D()(base_model.output)
    x = Dropout(0.3)(x)
    output = Dense(num_classes, activation='softmax')(x)

    model = Model(inputs=base_model.input, outputs=output)

    # Compile
    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

    # Callbacks
    early_stop = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)

    # Train
    history = model.fit(
        train_generator,
        validation_data=test_generator,
        epochs=3,
        callbacks=[early_stop],
        verbose=1
    )

    # Evaluate
    loss, acc = model.evaluate(test_generator, verbose=0)
    print(f"{model_name} Test Accuracy: {acc:.4f}")

    return model, acc, history

"""**train each model with 5 epochs**"""

# Number of classes from our generator
num_classes = train_generator.num_classes

# Train EfficientNetB0
efficientnet_model, efficientnet_acc, efficientnet_history = build_and_train_model(
    EfficientNetB0, num_classes=num_classes, model_name="EfficientNetB0"
)

# Train ResNet50
resnet_model, resnet_acc, resnet_history = build_and_train_model(
    ResNet50, num_classes=num_classes, model_name="ResNet50"
)

# Train DenseNet121
densenet_model, densenet_acc, densenet_history = build_and_train_model(
    DenseNet121, num_classes=num_classes, model_name="DenseNet121"
)

"""**compare models' results**"""

import pandas as pd

results_df = pd.DataFrame({
    "Model": ["EfficientNetB0", "ResNet50", "DenseNet121"],
    "Test Accuracy": [efficientnet_acc, resnet_acc, densenet_acc]
})

print(results_df)

def plot_history(history, model_name):
    plt.figure(figsize=(8,4))
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title(f"{model_name} Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()

plot_history(efficientnet_history, "EfficientNetB0")
plot_history(resnet_history, "ResNet50")
plot_history(densenet_history, "DenseNet121")